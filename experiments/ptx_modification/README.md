# Experiment: Validating Shared Memory Performance via Manual PTX Hacking

**Last Update:** 09/25/2025

## Objective

This directory contains a critical proof-of-concept experiment conducted in August 2025. After optimizing our Triton kernel to its limit in global memory (`OPv6`), we identified DRAM bandwidth as the final performance bottleneck.

The objective of this experiment was to **quantify the potential performance gain of moving the DP ring buffers (H, E, F) from global memory to low-latency shared memory**. Instead of immediately diving into complex compiler modifications, we opted to first validate this hypothesis by manually hacking the PTX assembly code generated by Triton.

## Methodology: The PTX Hack

1.  **Baseline:** We took the PTX assembly code generated by the vanilla Triton compiler for our most optimized kernel, `sw_kernel` (`OPv6` version).
2.  **Modification:** We manually edited this PTX file. The core modification involved:
    * Declaring a dynamic shared memory region (`.extern .shared`).
    * Expanding the number of registers declared in PTX.
    * Replacing all `ld.global`/`st.global` instructions associated with the H, E, and F buffers with their `ld.shared`/`st.shared` counterparts.
    * Manually implementing the pointer arithmetic and ring buffer logic for these shared memory buffers directly in PTX.

The result is a direct, apple-to-apple comparison between a global memory implementation and a shared memory implementation, running the exact same algorithm.

## Directory & File Guide

The key components for this experiment are organized as follows:

| Path                        | Description                                                                                                                                   |
| --------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------- |
| **`ptx/`** | Contains the PTX assembly files. The original, unmodified files are named `sw_kernel_OPv6.ptx`, while our manually hacked versions are suffixed with `_hack_HEF.ptx`. |
| **`scripts/`** | Contains the Python scripts used to launch and benchmark the PTX kernels via the CUDA Driver API.                                               |
| `scripts/benchmark_extz_ptx_0824_OPv6.py` | **(MAIN SCRIPT)** The stable script to run the correctness and performance comparison for the `OPv6` kernel and its hacked version. |
| `visualize/`                | **(Archive)** Contains plots and data from our mid-project analysis of the occupancy vs. performance trade-off. This is for historical reference and is not part of the final workflow. |

## How to Run & Validate

To replicate the experiment, run the primary benchmark script from the project root (`prototype_20250621/`).

```bash
python3 ptx_modification_0804/scripts/benchmark_extz_ptx_0824_OPv6.py
```

This script will:

1.  Load both the original `sw_kernel_OPv6.ptx` and the hacked `sw_kernel_OPv6_hack_HEF.ptx`.
2.  Execute both kernels on the same dataset.
3.  Perform a correctness check to ensure the outputs are identical.
4.  Print a side-by-side performance comparison of the two versions.

## How to Obtain Triton Compilation Artifacts (IR & PTX)

The baseline PTX files in this directory were generated using the Triton compiler's built-in debugging utilities. These environment variables are critical for analyzing the compilation process and for any future compiler development work.

  * `export TRITON_KERNEL_DUMP=1`

      * **Purpose:** When set, this directs the compiler to save all successful compilation artifacts to disk if the kernel compiles successfully. This includes various levels of Intermediate Representation (TTIR, TTGIR, LLVM IR), the final PTX assembly, and the compiled CUBIN file.

  * `export TRITON_DUMP_DIR=<path/to/dump/dir>`

      * **Purpose:** Specifies the output directory for the artifacts generated by `TRITON_KERNEL_DUMP`.

  * `export MLIR_ENABLE_DUMP=1`

      * **Purpose:** An essential flag for debugging compiler passes. It prints the state of the MLIR IR directly to the console **after each pass** in the pipeline. This allows you to trace exactly how the IR is transformed step-by-step.

  * `export TRITON_ALWAYS_COMPILE=1`

      * **Purpose:** Forces Triton to recompile the kernel on every run, ignoring any cached versions. This is necessary to ensure the dumping flags are effective for each execution.

### Example Usage:

To generate the PTX for the `OPv6` kernel, you would set these variables and run the main Triton benchmark script (not the PTX-specific one):

```bash
# Set the environment variables
export TRITON_KERNEL_DUMP=1
export TRITON_DUMP_DIR="./triton_dumps/opv6_dump"
export TRITON_ALWAYS_COMPILE=1

# Now, run the script that JIT-compiles the Triton kernel
# (This script is in the parent directory)
python3 benchmarking/scripts/benchmark_extz_OPv6.py
```

## Key Findings

The experiment was a success and yielded a crucial insight:

  * **Significant Performance Gain:** The manually hacked shared memory version demonstrated a **\~35% speedup** in kernel execution time compared to the highly-optimized global memory baseline (`OPv6`).
  * **Hypothesis Validated:** This result strongly confirmed that our kernel was indeed bound by DRAM bandwidth. The performance gain was achieved even though the large shared memory requirement reduced GPU occupancy (from 12 to 4 active blocks/SM), proving that the benefit of reduced memory latency far outweighed the cost of lower parallelism for this workload.